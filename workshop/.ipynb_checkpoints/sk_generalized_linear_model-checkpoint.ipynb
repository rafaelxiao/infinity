{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.  4.]\n",
      "-3.000000000000005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = [[0, 1], [2, 3], [3, 4.5]]\n",
    "y = [1, 3, 6]\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mind the problem of **multicollinearity**\n",
    "    - meaning the features are correlated with each other. The design matrix **X<span>** will close to singular\n",
    "    - the model with be **highly sensitive to random variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44444444 0.94444444]\n",
      "-0.08333333333333393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X = [[0, 1], [2, 3], [3, 4.5]]\n",
    "y = [1, 3, 6]\n",
    "\n",
    "reg = Ridge(alpha=0.5)\n",
    "reg.fit(X, y)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using alpha to controls the amount of shrinkage, thus make the model more robust to collinearity\n",
    "\n",
    "\n",
    "- L2 norm regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with built-in cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "import numpy as np\n",
    "\n",
    "X = [[0, 1], [2, 3], [3, 4.5]]\n",
    "y = [1, 3, 6]\n",
    "\n",
    "reg = RidgeCV(alphas=np.logspace(-6, 6, 13), cv=2) # set alpha, specify cv\n",
    "reg.fit(X, y)\n",
    "print(reg.alpha_) # the best alpha have been founded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84594595]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X = [[0, 1], [2, 3], [3, 4.5]]\n",
    "y = [1, 3, 6]\n",
    "\n",
    "reg = Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000)\n",
    "reg.fit(X, y)\n",
    "print(reg.predict([[1, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use L1 norm to get a sparse model, meaning driven the coefficient to 0\n",
    "\n",
    "\n",
    "- Often used for **feature selection**\n",
    "\n",
    "\n",
    "- Most often **preferable for high-dimensional datasets with many collinear features**\n",
    "\n",
    "\n",
    "- Model selection: built-in cv using **LassoCV** and **LassoLarsCV**\n",
    "    - **LassoLarsCV** based on **Least Angle Regression**, which exploring more relevant values of **alpha**, and ofter faster\n",
    "    - comparing to C of SVM, alpha = 1/C or alpha = 1/(n_samples*C)\n",
    "    \n",
    "    \n",
    "- Model selection: could also use **LassoLarsIC** to select model, which use ** Akaike information criterion (AIC)** and the **Bayes Information criterion (BIC)**, which considered a cheaper alternative to cross-validation, but need a proper estimation of degrees of freedom\n",
    "    - ** Akaike information criterion (AIC)**\n",
    "        - **-2L<sub>m</sub> + 2m**, L<sub>m</sub> is the maximized log-likelihood, m the number of parameters\n",
    "        - measures the goodness of fit\n",
    "        - the smaller the better\n",
    "    - **Bayes Information criterion (BIC)**\n",
    "        - **-2L<sub>m</sub> + ln(m)**, L<sub>m</sub> is the maximized log-likelihood, m the number of parameters\n",
    "        - usually better tha AIC\n",
    "        \n",
    "        \n",
    "- **MultiTaskLasso**, used when y in a 2D array of (n_samples, n_tasks)\n",
    "\n",
    "\n",
    "- **ElasticNet**, a linear regression trained with both L1 and L2 norm regularization, control the covex combination of L1 and L2 using **l1_ratio** parameter\n",
    "    - **ElasticNetCV**, which using cross-validation\n",
    "    - **MultiTaskElasticNet**, used for y in a 2D array of (n_samples, n_tasks)\n",
    "    - **MultiTaskElasticNetCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
