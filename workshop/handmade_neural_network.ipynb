{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We gonna make a neural network from scratch, based on numpy\n",
    "It will consist of several modules, eg. Linear, Activation, Optimization\n",
    "Each module included in the main structure of network, must have\n",
    "    - a forward method\n",
    "    - a backward method, which can only be called when the forward is performed\n",
    "'''\n",
    "\n",
    "class Linear:\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.bias = np.zeros(output_size)\n",
    "        self.prev = np.random.randn(output_size)\n",
    "        self.back_ready = False\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.input_size\n",
    "        out = x @ self.weights + self.bias\n",
    "        self.prev = x\n",
    "        self.back_ready = True\n",
    "        return out\n",
    "\n",
    "    def backward(self, g):\n",
    "        assert g.shape[1] == self.output_size\n",
    "        assert self.back_ready == True\n",
    "        weights_delta = self.prev.T @ g\n",
    "        bias_delta = g.mean(0)\n",
    "        g_prev = g @ self.weights.T\n",
    "        self.back_ready = False\n",
    "        return g_prev, weights_delta, bias_delta\n",
    "    \n",
    "    def update(self, weights_delta, bias_delta, lr=1e-4):\n",
    "        assert weights_delta.shape == self.weights.shape\n",
    "        assert bias_delta.shape == self.bias.shape\n",
    "        self.weights -= lr * weights_delta\n",
    "        self.bias -= lr * bias_delta\n",
    "\n",
    "class Relu:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.non_active = None\n",
    "        self.back_ready = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.non_active = x\n",
    "        self.back_ready = True\n",
    "        return x * (x>0) * (x<6)\n",
    "\n",
    "    def backward(self, g):\n",
    "        if self.back_ready:\n",
    "            assert g.shape == self.non_active.shape\n",
    "            out = 1 * (self.non_active>0) * (self.non_active<0) * g\n",
    "            self.non_active = None\n",
    "            self.back_ready = False\n",
    "            return out\n",
    "\n",
    "class Softmax:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.active = None\n",
    "        self.back_ready = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_value = np.max(x)\n",
    "        out = np.exp(x - max_value) / np.exp(x - max_value).sum()\n",
    "        self.back_ready = True\n",
    "        self.active = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, g):\n",
    "        if self.back_ready:\n",
    "            assert g.shape == self.active.shape\n",
    "            out = self.active - g\n",
    "            self.active = None\n",
    "            self.back_ready = False\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = load_digits()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "skf = StratifiedKFold(5)\n",
    "train_idx, test_idx = list(skf.split(X, y))[-1]\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Accuracy: 0.0761\n",
      "Epoch: 1001, Accuracy: 0.1014\n",
      "Epoch: 2001, Accuracy: 0.1408\n",
      "Epoch: 3001, Accuracy: 0.1577\n",
      "Epoch: 4001, Accuracy: 0.1775\n",
      "Epoch: 5001, Accuracy: 0.1915\n",
      "Epoch: 6001, Accuracy: 0.2056\n",
      "Epoch: 7001, Accuracy: 0.2197\n",
      "Epoch: 8001, Accuracy: 0.2507\n",
      "Epoch: 9001, Accuracy: 0.2873\n",
      "Epoch: 10001, Accuracy: 0.3042\n",
      "Epoch: 11001, Accuracy: 0.3239\n",
      "Epoch: 12001, Accuracy: 0.3408\n",
      "Epoch: 13001, Accuracy: 0.3380\n",
      "Epoch: 14001, Accuracy: 0.3606\n",
      "Epoch: 15001, Accuracy: 0.3887\n",
      "Epoch: 16001, Accuracy: 0.3944\n",
      "Epoch: 17001, Accuracy: 0.4028\n",
      "Epoch: 18001, Accuracy: 0.4000\n",
      "Epoch: 19001, Accuracy: 0.4028\n",
      "Epoch: 20001, Accuracy: 0.4056\n",
      "Epoch: 21001, Accuracy: 0.4056\n",
      "Epoch: 22001, Accuracy: 0.4169\n",
      "Epoch: 23001, Accuracy: 0.4169\n",
      "Epoch: 24001, Accuracy: 0.4197\n",
      "Epoch: 25001, Accuracy: 0.4225\n",
      "Epoch: 26001, Accuracy: 0.4254\n",
      "Epoch: 27001, Accuracy: 0.4338\n",
      "Epoch: 28001, Accuracy: 0.4366\n",
      "Epoch: 29001, Accuracy: 0.4366\n",
      "Epoch: 30001, Accuracy: 0.4366\n",
      "Epoch: 31001, Accuracy: 0.4394\n",
      "Epoch: 32001, Accuracy: 0.4394\n",
      "Epoch: 33001, Accuracy: 0.4451\n",
      "Epoch: 34001, Accuracy: 0.4507\n",
      "Epoch: 35001, Accuracy: 0.4507\n",
      "Epoch: 36001, Accuracy: 0.4479\n",
      "Epoch: 37001, Accuracy: 0.4507\n",
      "Epoch: 38001, Accuracy: 0.4479\n",
      "Epoch: 39001, Accuracy: 0.4479\n",
      "Epoch: 40001, Accuracy: 0.4535\n",
      "Epoch: 41001, Accuracy: 0.4535\n",
      "Epoch: 42001, Accuracy: 0.4535\n",
      "Epoch: 43001, Accuracy: 0.4507\n",
      "Epoch: 44001, Accuracy: 0.4535\n",
      "Epoch: 45001, Accuracy: 0.4563\n",
      "Epoch: 46001, Accuracy: 0.4563\n",
      "Epoch: 47001, Accuracy: 0.4592\n",
      "Epoch: 48001, Accuracy: 0.4563\n",
      "Epoch: 49001, Accuracy: 0.4592\n",
      "Epoch: 50001, Accuracy: 0.4563\n",
      "Epoch: 51001, Accuracy: 0.4563\n",
      "Epoch: 52001, Accuracy: 0.4620\n",
      "Epoch: 53001, Accuracy: 0.4592\n",
      "Epoch: 54001, Accuracy: 0.4592\n",
      "Epoch: 55001, Accuracy: 0.4563\n",
      "Epoch: 56001, Accuracy: 0.4592\n",
      "Epoch: 57001, Accuracy: 0.4563\n",
      "Epoch: 58001, Accuracy: 0.4563\n",
      "Epoch: 59001, Accuracy: 0.4592\n",
      "Epoch: 60001, Accuracy: 0.4563\n",
      "Epoch: 61001, Accuracy: 0.4563\n",
      "Epoch: 62001, Accuracy: 0.4563\n",
      "Epoch: 63001, Accuracy: 0.4563\n",
      "Epoch: 64001, Accuracy: 0.4620\n",
      "Epoch: 65001, Accuracy: 0.4648\n",
      "Epoch: 66001, Accuracy: 0.4676\n",
      "Epoch: 67001, Accuracy: 0.4676\n",
      "Epoch: 68001, Accuracy: 0.4648\n",
      "Epoch: 69001, Accuracy: 0.4648\n",
      "Epoch: 70001, Accuracy: 0.4676\n",
      "Epoch: 71001, Accuracy: 0.4676\n",
      "Epoch: 72001, Accuracy: 0.4676\n",
      "Epoch: 73001, Accuracy: 0.4648\n",
      "Epoch: 74001, Accuracy: 0.4676\n",
      "Epoch: 75001, Accuracy: 0.4648\n",
      "Epoch: 76001, Accuracy: 0.4676\n",
      "Epoch: 77001, Accuracy: 0.4676\n",
      "Epoch: 78001, Accuracy: 0.4676\n",
      "Epoch: 79001, Accuracy: 0.4676\n",
      "Epoch: 80001, Accuracy: 0.4648\n",
      "Epoch: 81001, Accuracy: 0.4648\n",
      "Epoch: 82001, Accuracy: 0.4676\n",
      "Epoch: 83001, Accuracy: 0.4704\n",
      "Epoch: 84001, Accuracy: 0.4704\n",
      "Epoch: 85001, Accuracy: 0.4704\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-54b8303e968a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mbatch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/athena/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2664\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prod_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2665\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = [\n",
    "    Linear(64, 32),\n",
    "    Relu(),\n",
    "    Linear(32, 32),\n",
    "    Relu(),\n",
    "    Linear(32, 10),\n",
    "    Softmax()\n",
    "]\n",
    "\n",
    "\n",
    "for i in range(100000):\n",
    "    \n",
    "    # Train\n",
    "    batch_idx = np.random.choice(range(len(X_train)), 32)\n",
    "\n",
    "    out = X_train[batch_idx] / 256\n",
    "    for m in model:\n",
    "        out = m.forward(out)\n",
    "\n",
    "    g = np.eye(10)\n",
    "    g = g[y_train[batch_idx]]\n",
    "    for m in model[::-1]:\n",
    "        if isinstance(m, Linear):\n",
    "            g, delta_weights, delta_bias = m.backward(g)\n",
    "            m.update(delta_weights, delta_bias)\n",
    "        else:\n",
    "            g = m.backward(g)\n",
    "    \n",
    "    #print(model[0].weights)\n",
    "    \n",
    "    # Test\n",
    "    if (i+1)%1000 == 0:\n",
    "        out = X_test / 256\n",
    "        \n",
    "        for m in model:\n",
    "            out = m.forward(out)\n",
    "        \n",
    "        print('Epoch: {}, Accuracy: {:.4f}'.format(i+1 ,(np.argmax(out, 1) == y_test).sum() / len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
