{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.  4.]\n",
      "-3.000000000000005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = [[0, 1], [2, 3], [3, 4.5]]\n",
    "y = [1, 3, 6]\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mind the problem of **multicollinearity**\n",
    "    - meaning the features are correlated with each other. The design matrix **X<span>** will close to singular\n",
    "    - the model with be **highly sensitive to random variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.44444444 0.94444444]\n",
      "-0.08333333333333393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X = [[0, 1], [2, 3], [3, 4.5]]\n",
    "y = [1, 3, 6]\n",
    "\n",
    "reg = Ridge(alpha=0.5)\n",
    "reg.fit(X, y)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using alpha to controls the amount of shrinkage, thus make the model more robust to collinearity\n",
    "- L2 norm regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with built-in cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "import numpy as np\n",
    "\n",
    "X = [[0, 1], [2, 3], [3, 4.5]]\n",
    "y = [1, 3, 6]\n",
    "\n",
    "reg = RidgeCV(alphas=np.logspace(-6, 6, 13), cv=2) # set alpha, specify cv\n",
    "reg.fit(X, y)\n",
    "print(reg.alpha_) # the best alpha have been founded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84594595]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X = [[0, 1], [2, 3], [3, 4.5]]\n",
    "y = [1, 3, 6]\n",
    "\n",
    "reg = Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000)\n",
    "reg.fit(X, y)\n",
    "print(reg.predict([[1, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use L1 norm to get a sparse model, meaning driven the coefficient to 0\n",
    "\n",
    "\n",
    "- Often used for **feature selection**\n",
    "\n",
    "\n",
    "- Most often **preferable for high-dimensional datasets with many collinear features**\n",
    "\n",
    "\n",
    "- Model selection: built-in cv using **LassoCV** and **LassoLarsCV**\n",
    "    - **LassoLarsCV** based on **Least Angle Regression**, which exploring more relevant values of **alpha**, and ofter faster\n",
    "    - comparing to C of SVM, alpha = 1/C or alpha = 1/(n_samples*C)\n",
    "    \n",
    "    \n",
    "- Model selection: could also use **LassoLarsIC** to select model, which use ** Akaike information criterion (AIC)** and the **Bayes Information criterion (BIC)**, which considered a cheaper alternative to cross-validation, but need a proper estimation of degrees of freedom\n",
    "    - ** Akaike information criterion (AIC)**\n",
    "        - **-2L<sub>m</sub> + 2m**, L<sub>m</sub> is the maximized log-likelihood, m the number of parameters\n",
    "        - measures the goodness of fit\n",
    "        - the smaller the better\n",
    "    - **Bayes Information criterion (BIC)**\n",
    "        - **-2L<sub>m</sub> + ln(m)**, L<sub>m</sub> is the maximized log-likelihood, m the number of parameters\n",
    "        - usually better tha AIC\n",
    "        \n",
    "        \n",
    "- **MultiTaskLasso**, used when y in a 2D array of (n_samples, n_tasks)\n",
    "\n",
    "\n",
    "- **ElasticNet**, a linear regression trained with both L1 and L2 norm regularization, control the covex combination of L1 and L2 using **l1_ratio** parameter\n",
    "    - **ElasticNetCV**, which using cross-validation\n",
    "    - **MultiTaskElasticNet**, used for y in a 2D array of (n_samples, n_tasks)\n",
    "    - **MultiTaskElasticNetCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Least Angle Regression (LARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use **LARS** function\n",
    "- Used to **select features**, similar to forward stepwise regression and forward stagewise regression\n",
    "- Pros: faster, easily modified\n",
    "- Cons: sensitive to noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LARS Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97823825]\n",
      "[0.         1.28459732]\n",
      "[[0.         0.        ]\n",
      " [0.         3.19001149]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoLars\n",
    "\n",
    "X = [[0, 1], [2, 3], [3, 4.5]]\n",
    "y = [1, 3, 6]\n",
    "\n",
    "reg = LassoLars(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000)\n",
    "reg.fit(X, y)\n",
    "print(reg.predict([[1, 1]]))\n",
    "print(reg.coef_)\n",
    "print(reg.coef_path_) # Show the path of developing coefficients, which has the size (n_features, max_features+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal Matching Pursuit (OMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use **OrthogonalMatchingPursuit** and **OrthogonalMatchingPursuitCV** function\n",
    "- Similar as LARS, which performs **feature selection** and get a sparse model\n",
    "- Different in perform **L0 norm** regularization, so the number of non-zero coefficients must be specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introducing uninformative priors for regularization\n",
    "- Similar to Ridge Regression\n",
    "- Use data at hand to fit the regularization parameters\n",
    "- Inference could be time consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.61066477]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "X = [[0, 1], [2, 3], [3, 4.5]]\n",
    "y = [1, 3, 6]\n",
    "\n",
    "reg = BayesianRidge()\n",
    "reg.fit(X, y)\n",
    "print(reg.predict([[2, 3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The parameters **w<span>**, **alpha**, **lambda** are inferred\n",
    "- The hyper parameters, which are the priors of **alpha**, **lambda** could be specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Relevance Determination (ARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use **ARDRegression** function\n",
    "- Similar to Bayesian Ridge Regression, but produce a sparser model\n",
    "    - It drops the assumption of spherical Gaussian priors for **w<span>**\n",
    "    - It assigns a standard deviation **lambda** for each **w<span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Toy Example for Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Regression\n",
      "mse: 22.0987\n",
      "\n",
      "\n",
      "Ridge Regression\n",
      "mse: 22.1422 (alpha: 0.10)\n",
      "mse: 22.1867 (alpha: 0.20)\n",
      "mse: 22.2305 (alpha: 0.30)\n",
      "mse: 22.2727 (alpha: 0.40)\n",
      "mse: 22.3129 (alpha: 0.50)\n",
      "\n",
      "\n",
      "Ridge CV\n",
      "mse: 22.1422 (alpha: 0.10)\n",
      "\n",
      "\n",
      "Lasso Regression\n",
      "mse: 23.3859 (alpha: 0.10)\n",
      "mse: 23.4004 (alpha: 0.20)\n",
      "mse: 23.2705 (alpha: 0.30)\n",
      "mse: 23.2139 (alpha: 0.40)\n",
      "mse: 23.2303 (alpha: 0.50)\n",
      "\n",
      "\n",
      "Lasso CV\n",
      "mse: 23.3859 (alpha: 0.10)\n",
      "\n",
      "\n",
      "ElasticNet Regression\n",
      "mse: 22.9952 (alpha: 0.10)\n",
      "mse: 22.8678 (alpha: 0.20)\n",
      "mse: 22.8932 (alpha: 0.30)\n",
      "mse: 22.9980 (alpha: 0.40)\n",
      "mse: 23.1302 (alpha: 0.50)\n",
      "\n",
      "\n",
      "ElasticNet CV\n",
      "mse: 22.9952 (alpha: 0.10)\n",
      "\n",
      "\n",
      "LARS\n",
      "mse: 22.0987\n",
      "\n",
      "\n",
      "LARS Lasso\n",
      "mse: 29.3274 (alpha: 0.10)\n",
      "mse: 40.9072 (alpha: 0.20)\n",
      "mse: 58.8988 (alpha: 0.30)\n",
      "mse: 72.2608 (alpha: 0.40)\n",
      "mse: 72.2608 (alpha: 0.50)\n",
      "\n",
      "\n",
      "LARS Lasso CV\n",
      "mse: 22.0987 (alpha: 0.00)\n",
      "\n",
      "\n",
      "LARS Lasso IC\n",
      "mse: 24.3174 (alpha: 0.01 using aic)\n",
      "mse: 26.2103 (alpha: 0.06 using bic)\n",
      "\n",
      "\n",
      "OMP\n",
      "mse: 23.0326\n",
      "[-1.28289621e-01  2.10004125e-02  0.00000000e+00  2.98248555e+00\n",
      " -1.82441129e+01  4.41376500e+00  0.00000000e+00 -1.33077415e+00\n",
      "  1.19276579e-01  0.00000000e+00 -9.65561610e-01  1.34083426e-02\n",
      " -5.23704827e-01]\n",
      "\n",
      "\n",
      "OMP CV\n",
      "mse: 26.0365\n",
      "[ 0.          0.          0.          0.          0.          4.73541072\n",
      "  0.         -0.64487718  0.          0.         -0.86179019  0.01546438\n",
      " -0.61809449]\n",
      "\n",
      "\n",
      "Bayesian Ridge Regression\n",
      "mse: 22.9306\n",
      "[-0.12150925  0.03444806 -0.01430184  1.74271072 -1.47372643  4.03661964\n",
      " -0.01810368 -1.18469646  0.24515591 -0.01217638 -0.76764028  0.01368767\n",
      " -0.57795496]\n",
      "\n",
      "\n",
      "ARD\n",
      "mse: 23.3196\n",
      "[-1.04433952e-01  9.38986654e-03  0.00000000e+00  2.74124654e+00\n",
      " -1.62790362e+01  4.48734816e+00  0.00000000e+00 -1.17830849e+00\n",
      "  8.64963100e-02  0.00000000e+00 -9.44309276e-01  1.25460156e-02\n",
      " -5.28859370e-01]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, \\\n",
    "ElasticNet, ElasticNetCV, Lars, LarsCV, LassoLars, LassoLarsCV, LassoLarsIC, \\\n",
    "OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV, BayesianRidge, ARDRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare the data\n",
    "np.random.seed(42)\n",
    "data = load_boston()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Parameters\n",
    "alpha = np.arange(0.1, 0.6, 0.1)\n",
    "l1_ratio = 0.5\n",
    "\n",
    "# OLS regression\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print('OLS Regression')\n",
    "print('mse: %.4f'%(mean_squared_error(y_test, clf.predict(X_test))))\n",
    "print('\\n')\n",
    "\n",
    "# Ridge regression\n",
    "print(\"Ridge Regression\")\n",
    "for i in alpha:\n",
    "    clf = Ridge(alpha=i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('mse: %.4f (alpha: %.2f)'%(mean_squared_error(y_test, clf.predict(X_test)), i))\n",
    "print('\\n')\n",
    "\n",
    "# Ridge CV\n",
    "print(\"Ridge CV\")\n",
    "clf = RidgeCV(alphas=alpha)\n",
    "clf.fit(X_train, y_train)\n",
    "print('mse: %.4f (alpha: %.2f)'%(mean_squared_error(y_test, clf.predict(X_test)), clf.alpha_))\n",
    "print('\\n')\n",
    "\n",
    "# Lasso regression\n",
    "print(\"Lasso Regression\")\n",
    "for i in alpha:\n",
    "    clf = Lasso(alpha=i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('mse: %.4f (alpha: %.2f)'%(mean_squared_error(y_test, clf.predict(X_test)), i))\n",
    "print('\\n')\n",
    "\n",
    "# Lasso CV\n",
    "print(\"Lasso CV\")\n",
    "clf = LassoCV(alphas=alpha, cv=3)\n",
    "clf.fit(X_train, y_train)\n",
    "print('mse: %.4f (alpha: %.2f)'%(mean_squared_error(y_test, clf.predict(X_test)), clf.alpha_))\n",
    "print('\\n')\n",
    "\n",
    "# ElasticNet regression\n",
    "print(\"ElasticNet Regression\")\n",
    "for i in alpha:\n",
    "    clf = ElasticNet(alpha=i, l1_ratio=l1_ratio)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('mse: %.4f (alpha: %.2f)'%(mean_squared_error(y_test, clf.predict(X_test)), i))\n",
    "print('\\n')\n",
    "\n",
    "# ElasticNet CV\n",
    "print(\"ElasticNet CV\")\n",
    "clf = ElasticNetCV(alphas=alpha, cv=3, l1_ratio=l1_ratio)\n",
    "clf.fit(X_train, y_train)\n",
    "print('mse: %.4f (alpha: %.2f)'%(mean_squared_error(y_test, clf.predict(X_test)), clf.alpha_))\n",
    "print('\\n')\n",
    "\n",
    "# Least Angle Regression (LARS)\n",
    "print(\"LARS\")\n",
    "clf = Lars()\n",
    "clf.fit(X_train, y_train)\n",
    "print('mse: %.4f'%(mean_squared_error(y_test, clf.predict(X_test))))\n",
    "print('\\n')\n",
    "\n",
    "# LARS Lasso\n",
    "print(\"LARS Lasso\")\n",
    "for i in alpha:\n",
    "    clf = LassoLars(alpha=i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('mse: %.4f (alpha: %.2f)'%(mean_squared_error(y_test, clf.predict(X_test)), i))\n",
    "print('\\n')\n",
    "\n",
    "# LARS Lasso CV\n",
    "print(\"LARS Lasso CV\")\n",
    "clf = LassoLarsCV(cv=3) # Find the alpha, no need to specify\n",
    "clf.fit(X_train, y_train)\n",
    "print('mse: %.4f (alpha: %.2f)'%(mean_squared_error(y_test, clf.predict(X_test)), clf.alpha_))\n",
    "print('\\n')\n",
    "\n",
    "# LARS Lasso IC\n",
    "print(\"LARS Lasso IC\")\n",
    "clf = LassoLarsIC(criterion='aic') # Use IC to find the best alpha\n",
    "clf.fit(X_train, y_train)\n",
    "print('mse: %.4f (alpha: %.2f using aic)'%(mean_squared_error(y_test, clf.predict(X_test)), clf.alpha_))\n",
    "clf = LassoLarsIC(criterion='bic') # Use IC to find the best alpha\n",
    "clf.fit(X_train, y_train)\n",
    "print('mse: %.4f (alpha: %.2f using bic)'%(mean_squared_error(y_test, clf.predict(X_test)), clf.alpha_))\n",
    "print('\\n')\n",
    "\n",
    "# Orthogonal Matching Pursuit (OMP)\n",
    "print(\"OMP\")\n",
    "clf = OrthogonalMatchingPursuit(n_nonzero_coefs=10) # Must specify how many non-zero coefficients \n",
    "clf.fit(X_train, y_train)\n",
    "print('mse: %.4f'%(mean_squared_error(y_test, clf.predict(X_test))))\n",
    "print(clf.coef_)\n",
    "print('\\n')\n",
    "\n",
    "# OMP CV\n",
    "print(\"OMP CV\")\n",
    "clf = OrthogonalMatchingPursuitCV(cv=3) # Use CV to find the best non-zero numbers\n",
    "clf.fit(X_train, y_train)\n",
    "print('mse: %.4f'%(mean_squared_error(y_test, clf.predict(X_test))))\n",
    "print(clf.coef_)\n",
    "print('\\n')\n",
    "\n",
    "# Bayesian Ridge Regression\n",
    "print(\"Bayesian Ridge Regression\")\n",
    "clf = BayesianRidge() # See, most parameters are inferred\n",
    "clf.fit(X_train, y_train)\n",
    "print('mse: %.4f'%(mean_squared_error(y_test, clf.predict(X_test))))\n",
    "print(clf.coef_)\n",
    "print('\\n')\n",
    "\n",
    "# Automatic Relevance Determination\n",
    "print(\"ARD\")\n",
    "clf = ARDRegression() # See, it will be sparser than the Bayesian Ridge\n",
    "clf.fit(X_train, y_train)\n",
    "print('mse: %.4f'%(mean_squared_error(y_test, clf.predict(X_test))))\n",
    "print(clf.coef_)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need to specify L1, L2, elasticnet or None regularizer\n",
    "\n",
    "\n",
    "- Use different **solvers** to specify different regularizer, as well as **OVR(one-vs-rest)** or **multinomial**\n",
    "     - **liblinear**: OVR, L1, L2\n",
    "     - **lbfgs**, **sag**, **newton-cg**: OVR, multinomial, L2, faster for high-dimensional data\n",
    "         - **sag** best for datasets with large samples and large number of features\n",
    "         - **lbfgs** is recomended for small datasets\n",
    "     - **saga**: OVR, multinomial, L1, L2, elasticnet(the only support), a variant of **sag**\n",
    "     \n",
    "     \n",
    "- Use **LogisticRegressionCV** to find optimal **C<span>** and **l1_ratio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Has **SGDClassifier** and **SGDRegressor**\n",
    "- Efficient for large dataset\n",
    "- Use **partial_fit** to enable online/out-of-core learning\n",
    "- Setting **hinge** loss, yielding a SVM, while **log** loss produce a logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passive Aggressive Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dealing with errors and outliers\n",
    "- **RANSAC**, **Theil Sen**, **HuberRegressor**\n",
    "- When in doubt, use **RANSAC** (RANdom SAmple Consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use **PloynomialFeatures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
