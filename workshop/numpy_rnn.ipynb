{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = open('../../datasets/shakespeare_input.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 4573338 characters, 67 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('The data has %d characters, %d unique'%(data_size, vocab_size))\n",
    "char_to_ix = {char:i for i, char in enumerate(chars)}\n",
    "ix_to_char = {i:char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "        loss += -np.log(ps[t][targets[t], 0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-10f83e423dca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "xs = np.zeros((vocab_size, 1))\n",
    "xs[3] = 1\n",
    "hs = np.dot(Wxh, xs)\n",
    "ys = np.dot(Why, hs)\n",
    "ps = np.exp(ys) / np.sum(np.exp(ys))\n",
    "-np.log(ps[xs, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01933695],\n",
       "       [0.00499019],\n",
       "       [0.01598115],\n",
       "       [0.01775223],\n",
       "       [0.01375716],\n",
       "       [0.01793488],\n",
       "       [0.0260355 ],\n",
       "       [0.01732183],\n",
       "       [0.00508947],\n",
       "       [0.01017336],\n",
       "       [0.02711037],\n",
       "       [0.00347543],\n",
       "       [0.02292304],\n",
       "       [0.01140837],\n",
       "       [0.00649978],\n",
       "       [0.01463681],\n",
       "       [0.01844737],\n",
       "       [0.01249266],\n",
       "       [0.00872504],\n",
       "       [0.01365543],\n",
       "       [0.01749172],\n",
       "       [0.0034211 ],\n",
       "       [0.00999599],\n",
       "       [0.02004093],\n",
       "       [0.01648816],\n",
       "       [0.01323914],\n",
       "       [0.00471925],\n",
       "       [0.02433674],\n",
       "       [0.0262029 ],\n",
       "       [0.01641132],\n",
       "       [0.0035357 ],\n",
       "       [0.02007434],\n",
       "       [0.00275762],\n",
       "       [0.01614178],\n",
       "       [0.02488153],\n",
       "       [0.00408863],\n",
       "       [0.02706315],\n",
       "       [0.01939588],\n",
       "       [0.01914321],\n",
       "       [0.00426945],\n",
       "       [0.0218198 ],\n",
       "       [0.0021903 ],\n",
       "       [0.00894679],\n",
       "       [0.01652579],\n",
       "       [0.01021793],\n",
       "       [0.01350675],\n",
       "       [0.01469191],\n",
       "       [0.01329893],\n",
       "       [0.02148435],\n",
       "       [0.02640528],\n",
       "       [0.0156784 ],\n",
       "       [0.01826987],\n",
       "       [0.02057114],\n",
       "       [0.01927495],\n",
       "       [0.01028592],\n",
       "       [0.02374417],\n",
       "       [0.01992136],\n",
       "       [0.01880645],\n",
       "       [0.02433787],\n",
       "       [0.00341567],\n",
       "       [0.00632104],\n",
       "       [0.02001322],\n",
       "       [0.00322754],\n",
       "       [0.02372849],\n",
       "       [0.00677305],\n",
       "       [0.01541492],\n",
       "       [0.01968256]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " OLDs'u'ZBAOkM-B.mmR GdyATIXG--ZMZEdOJhuMhFPGpsjMXaG-..eTzW]LtzFRH,'jK;PvWa iGthOerjBGBjhDTPNpZs-Z$E$HLfRqL]e-$&]WZvyMv]aZq,qhHFgs tl.wkbt.EW,[[AEz;D:.f\n",
      "qr&,.'hNoRok\n",
      "NyAt?beb&QfX'Yg$WXdigVp'Dr?HnltpXE$ \n",
      "----\n",
      "iter 0, loss: 105.117313\n",
      "----\n",
      " hcs eatl etn\n",
      "e unnh br S  i rrhhF:-ssIse\n",
      "aeegeAo\n",
      "u :khizs oetnenoend  na'ts:mresbenahy h hls rrstn akOtlMye\n",
      "en etedwWrtn  hs\n",
      "eosWuntn F t hht,caiwlbr su nSwhat nti dnohe iamthy lrhpL aonay re nLonrno  \n",
      "----\n",
      "iter 100, loss: 105.080098\n",
      "----\n",
      " entvcr devuv iscotnil,ulgs  fodialaT:ee f:eivcnolfeaOgTeazogee ui e.e\n",
      "Iore oyhras lhhss hhhs :\n",
      "heevbTohoerS\n",
      " ll tna\n",
      "lyr s  cCpheelooge\n",
      ",Wkwitt htYplsheetc o nklh:oel? Tee incohe\n",
      "]Craa  brmhllhe diGlre \n",
      "----\n",
      "iter 200, loss: 103.240845\n",
      "----\n",
      " nnobS,ss\n",
      "btigoCI,e ky ar cr usMjah wVeeh splhCiviueir obu w sul\n",
      "nwWiumre Rh tr,\n",
      "A: Usndrafr:la\n",
      "mgA C vfr  ,eS' \n",
      " ogzsee  s'a  rhs T-NBbIIfeod rgoneh\n",
      "UNQb i toert gobCvWese\n",
      "t At t d  IL\n",
      "tsnaCbtib'ast f \n",
      "----\n",
      "iter 300, loss: 101.256366\n",
      "----\n",
      " . \n",
      "Tddmo ls-bh:rrrs fith tHires  Etieenus lsoiethe re thabyr\n",
      "xhalulm Thor \n",
      "iFh. Tit'lu nou thourd ,sutohan:m pftheslnngo s chnee: uu waThd nriFha uihoen :bysva;lyhs roteegs! akislhe urome fe te fe tau \n",
      "----\n",
      "iter 400, loss: 99.049098\n",
      "----\n",
      " es e filh toe:\n",
      "yA mBaIo. lotheo naothe , m\n",
      "Foqnthe ahr., tgowelcuni mIoO, eitt teawhn scome;ce hs anUnai'ou dlatin lunaee s ouwT mathe s ni m:,\n",
      "\n",
      "AM!byhivgu d se,\n",
      "ouenen f, w sornsn ls bo tisodg\n",
      "derces \n",
      "----\n",
      "iter 500, loss: 96.967318\n",
      "----\n",
      " wz\n",
      "\n",
      "Hzor tiza gre\n",
      " ghp aa me uiaifmar th\n",
      "Ter  heoFeAofaait syvithe wers, loLl doropb, :f:\n",
      "Nokiw,g : doarseghee te pr hipghehaeaonodo'inh grls dour theafutuWwukf h owe bnhe pn an tossmoveay theiin ch . \n",
      "----\n",
      "iter 600, loss: 94.616884\n",
      "----\n",
      "  ronys tut' febe theolv ffenwom untony wr t dhes sun cas. mheh, raridaw,t f w te:.\n",
      "The,:\n",
      "\n",
      "AVLKC-y, whetenis nmmud fou wit fir aid od ioweisal d iml othoEamerslinette biCywoud lnsher g tok u mhl\n",
      "Tf hat \n",
      "----\n",
      "iter 700, loss: 92.465326\n",
      "----\n",
      " es mwa tolt yonhed\n",
      "\n",
      "IFAATaof theomhavanobtouiy, ase ho.lou the aidtciel. thed t, oritoyee lhor gid\n",
      "wnt portoTouy o phe p.:\n",
      "loave , uls tertpes aWa moug  loreel,ee  oonrre ereeou\n",
      "\n",
      "kW]IM?il.\n",
      "Toret, fotf \n",
      "----\n",
      "iter 800, loss: 90.130584\n",
      "----\n",
      "  toses uldhey pothofd his mU neaat sh thpsoustithousen, amn heanss athineytonn: .ro, wr lh th\n",
      "unl thard ; oitherapsrsn cese filw;y iokg nn twonot irke:\n",
      "ViYg dallnk unsll iwns s I shim hes. nons mofds\n",
      " \n",
      "----\n",
      "iter 900, loss: 87.880342\n",
      "----\n",
      " r thean shand ves ot scthelmrels\n",
      "M!acr ther' ywaf swurd ansge.\n",
      "Hicould cy hor loceore  t ae can, ir roird nlarnbselesd iulsd hoor sher uI ilir palars.\n",
      "I t:Xr.\n",
      "\n",
      "ikans. aursoank t, oue.\n",
      "qkild wiar thoih \n",
      "----\n",
      "iter 1000, loss: 85.871297\n",
      "----\n",
      " or wiimss ancer teerd ghither,\n",
      "Tee bis Aid t: to aed bocausb\n",
      "Ao; eup' wenre-\n",
      "The 'eto hamyin ce Aaug dvilf,\n",
      "Alo She t M heu d.\n",
      "Meor the thimey thesop soatf Crk thurd anwtiloklase illl the te yout  tha \n",
      "----\n",
      "iter 1100, loss: 83.696073\n",
      "----\n",
      " ige t mooc;lk\n",
      "cires kountushe, aeteyk wher thy welt sorour lpotfI s rid mepu tores n te? lncat,\n",
      "CIUCICIUS&L:\n",
      "The otika. yt shy  ill\n",
      "Cu'nl\n",
      "Cid'ar, Tnvers c ares ko thasom boware.\n",
      "We shar on tirs.\n",
      ":? bo \n",
      "----\n",
      "iter 1200, loss: 81.750379\n",
      "----\n",
      " tlmafe cnher,\n",
      "Ove galle wond antsi\n",
      "claobo\n",
      " mid Ier nheromtietthatheath, hof Sthus;on dand, worisnd, I alars thin htuned \n",
      "MoFThaad pi hdess anditt l wular,\n",
      "\n",
      "S:\n",
      "\n",
      "MAA-MMS:\n",
      "\n",
      "MUS:\n",
      "Wfiind I o ceoud aon mava \n",
      "----\n",
      "iter 1300, loss: 79.921675\n",
      "----\n",
      " ogs imy ou singulls!s\n",
      "mict  gresilc rire o'-,\n",
      "Thut, the phe ronge the SeOgln yokg Iolorct theR r? si wfind we ga, Iils t sonc hand tcetd han; bolst gh'd cen'med wessd thav'dieged s'se\n",
      "Thanr\n",
      "d\n",
      "bep wlat \n",
      "----\n",
      "iter 1400, loss: 78.414533\n",
      "----\n",
      " eminghot hoyl. Oir nis,\n",
      "An?\n",
      "?\n",
      "Thely n'th to- at, pn finp\n",
      "\n",
      "MzEINgel\n",
      "S:\n",
      "MMI\n",
      "NvUS:\n",
      "YIUS:\n",
      "I\n",
      "MeFchayd,\n",
      "Baty of owre horr Fincamive thishetp sk?\n",
      "Solt fy nore pate gfely cfand epase; ans ther mive lyoi thood \n",
      "----\n",
      "iter 1500, loss: 76.811722\n",
      "----\n",
      " he.\n",
      "Bowd kasgom gele per wo nerol thess a, ofy thol!.\n",
      "Tfir' sh, fushaprye pote be hore odes hotin ocitinc,\n",
      "'d batousde sachuelcncird in\n",
      "uel slere  fouios hens\n",
      "Nun\n",
      "ja, mwinclill, che ilegid I'eicetheve \n",
      "----\n",
      "iter 1600, loss: 75.464723\n",
      "----\n",
      "  he in, theg heeqyr, ioldt, watt tined,\n",
      "\n",
      "je hihe thond fafe:.\n",
      "\n",
      "BLrou mowid muns oasr pumiss cous inesy ifisg thethe ies, hicn athis wheen an n\n",
      "Vhe, doV thepebenold iucaghnifse therohe ce,\n",
      "yong benadce \n",
      "----\n",
      "iter 1700, loss: 74.106418\n",
      "----\n",
      " wnoe tanlenfove: IN;The hor\n",
      "\n",
      "MEALAIUINTUS:\n",
      "EAL IUS:\n",
      "Howss hunhe huandabm Mothang, I thepd nomkifom:\n",
      "Anisi, telheg:\n",
      "OoorRtis! allWhafhepof molom ansapgilasle shathivee Ry Thorald; cheve roskn isee! yor \n",
      "----\n",
      "iter 1800, loss: 72.957908\n",
      "----\n",
      "  ecbath harent diles imayiJly tat winansthog tor hamnins. I pe pancre:\n",
      "Ik g ipusce tas\n",
      "!is and, caplid ote no th arrellor linrthens\n",
      "Hovid thes\n",
      "Oups!\n",
      "Cfetis.\n",
      "b haf it, and withe\n",
      "\n",
      "TVKUS:\n",
      "Tt she kas: lli \n",
      "----\n",
      "iter 1900, loss: 71.651464\n",
      "----\n",
      " an. wo.\n",
      "Th Atowdor\n",
      "The tor!\n",
      "MatlWe gere birs,e theachaapd hernget aly\n",
      "Yot, hache ang Tousltrand to chaves kerthe bate\n",
      "-llt, arche,\n",
      "\n",
      "UDUS:\n",
      "He m wcofd the ham bthergige ereleavallit and citr aldthhe co  \n",
      "----\n",
      "iter 2000, loss: 70.533235\n",
      "----\n",
      " urc ad pegeno?vid he glaze'kk!\n",
      "\n",
      "Shbte s pice ning sonet thimeth Iapln\n",
      "Asee seat: ook wounges orsees: wofe Aan; gide,\n",
      "Win'e\n",
      "Boofe tuem mide thora khekeY'e:\n",
      "Hare: ff, rakeas the th fild,\n",
      " IUS:\n",
      "CR:\n",
      "Awlr\n",
      " \n",
      "----\n",
      "iter 2100, loss: 69.635216\n",
      "----\n",
      " e wing ury ome pe he\n",
      "Yo s alg.\n",
      "Muve bet to homind tood jitg erell\n",
      "Buat andees s, To peR oom lirdeoce\n",
      "\n",
      "I DUTUS:\n",
      "Mprary me am, wicoy Wow wous ow pbu hamz throus wons tyous, leceshe Co liris at insend id \n",
      "----\n",
      "iter 2200, loss: 68.476336\n",
      "----\n",
      " e\n",
      "I were thee the dke mat Mamy,\n",
      "Lniy bee mito folisl\n",
      "S:\n",
      "O I cublt.\n",
      "\n",
      "AThandin in\n",
      "bl.\n",
      "Hilu ereir\n",
      "\n",
      "vemams Ium tomve somithed pe onspyo ksy non yorentwe bor I is torrosas  is ly en tteu ht moy fam mot. I  \n",
      "----\n",
      "iter 2300, loss: 67.718783\n",
      "----\n",
      " borvemar cod y ie cy: loromve Cyoui ci'cse atre oop. ans told ritcipg iurs yourcal co'ny your cit Iay, t vour \n",
      "aris pestad do. tats, sCUur'n venouply homiy gr mo thic shiveve bfyimln\n",
      "Yot nhougllld brb \n",
      "----\n",
      "iter 2400, loss: 66.725826\n",
      "----\n",
      " tou cu the pom.\n",
      "\n",
      "Buy y he bu ; h ?Brsimy he mould ilc ae, reips walt shotd\n",
      "Malis.\n",
      "\n",
      "Bind heei ntrt noive Cyacare ofts yof, sho rell goi pyone.\n",
      "Bearis wou fot entileds rours socor you thiein lly dill to \n",
      "----\n",
      "iter 2500, loss: 65.847849\n",
      "----\n",
      " oind ave he izele Myofom. , FeOreds thes mestour I an: onemits yove .\n",
      "Snesthen thave wead co fo cet thed.\n",
      "Woleng, do.\n",
      "Waclthim Hen: ind mizst-Cal]en thet, vovius? yoen misn coungret; mod yputh in!\n",
      "wha \n",
      "----\n",
      "iter 2600, loss: 65.104458\n",
      "----\n",
      " o Cidt an der: af herat noudrar me of gom wog serom se und tird cufett. Tcoisgunsu phe titonte tunme huze Fhincein.\n",
      ". frimee on hid thel; uree ive this fo tund srage\n",
      "zon Cothin erd\n",
      "To be cagtirsy son  \n",
      "----\n",
      "iter 2700, loss: 64.411004\n",
      "----\n",
      " art hous codreinfe cat tten wy Yor ortt thatwe bllant.\n",
      "\n",
      "CORSINIUS:\n",
      "Ond Tate nou der oulle pealt youl 'siy,\n",
      "Sha wows af.: af os; sithtt, wrulid bder pall\n",
      "\n",
      "Sar wen tar 'ish bubsth,\n",
      "Whell ine hi ovece we \n",
      "----\n",
      "iter 2800, loss: 63.344038\n",
      "----\n",
      " e them, on bowerow tinnusr aps!\n",
      "There lalle the cull bele thed beh, whilen yo ceicfame agore poferacin,'spone yhot ackiold sherhtcour, I Yomit,. tnort, sheat- hoved tourtitcer hapt qipgime alemakugenc \n",
      "----\n",
      "iter 2900, loss: 62.813453\n",
      "----\n",
      "  whanp' enbuif yo is you be tous en, andere\n",
      "\n",
      "TI IUS:\n",
      "Tout sh,\n",
      "Whete volpte he toon thele whawde co weive acir youd katith tgat\n",
      "Thatt iine er to d ase has:, fbat chith beike bon on, he ther wily witpon \n",
      "----\n",
      "iter 3000, loss: 62.301949\n",
      "----\n",
      " eid yo titou!\n",
      "\n",
      "CUS onis,\n",
      "One s aco dumhim maftwer Rate;\n",
      "S:\n",
      "IDHeend ites\n",
      "Whid citreen thesthe.\n",
      "\n",
      "ACiRUS:\n",
      "\n",
      "CRNOUSNUS: Authan wher alde tou tore beave fher!\n",
      "Wham thatillo ke,\n",
      "\n",
      "Toululnse wor, thallem whyel \n",
      "----\n",
      "iter 3100, loss: 61.767078\n",
      "----\n",
      " kats:\n",
      "ld ar.\n",
      "\n",
      "BENTUS:\n",
      "OL.\n",
      "San wowe es ounseon?\n",
      "H:\n",
      "Ifaw. dis to y't theake.\n",
      "\n",
      "NENENENINOUS:\n",
      "Whe:\n",
      "S\n",
      "Theoa youm nare her the beiver uuse bent woninme leit rat thon.\n",
      "Th ante\n",
      "Whe\n",
      "Wher, ande gh ored wale\n",
      "The \n",
      "----\n",
      "iter 3200, loss: 61.134469\n",
      "----\n",
      " ns?\n",
      "\n",
      "Sritr,,\n",
      "\n",
      "FjUS:\n",
      "Ald\n",
      "No'sh Lave of le fildde'd'-t go! suty and touess, ould:\n",
      "Toen.:\n",
      "Whe vot Ceig hedtom gom he ir thes.\n",
      "\n",
      "CUSIS:\n",
      "Thatme pous iagure hory at biclat dichicoll\n",
      "dob cod hese.\n",
      "\n",
      "LECEMIUS:\n",
      " \n",
      "----\n",
      "iter 3300, loss: 60.697701\n",
      "----\n",
      " , hals venhat on not; bur they the, ego peisetre, I'y hat t tha y, hest mither vat noben wrgowert youl'.\n",
      "\n",
      "CRRNENENIUS:\n",
      "Fo topent Iom shet on tour\n",
      "\n",
      "fORTONICIOLIOLANIILA:\n",
      "MNIUS:\n",
      "Yore\n",
      "MENote butort oos s \n",
      "----\n",
      "iter 3400, loss: 60.072608\n",
      "----\n",
      " I Of Cozend bt po sors,\n",
      "\n",
      "MENIUS:\n",
      "Thas\n",
      "Blomt\n",
      "Omds\n",
      "Ar they antlt sl?\n",
      "Ace shas, wsory, sire whfas yore fou tr,\n",
      "Yon theet yous or;\n",
      "Rhittit halesis be vetare whe, ir tot wasetht The ar trest.\n",
      "\n",
      "vome lthay t \n",
      "----\n",
      "iter 3500, loss: 59.589804\n",
      "----\n",
      " Ou,\n",
      "Ar tor t fir ant,\n",
      "And hyald yourtit Io, thinso.\n",
      "\n",
      "The.\n",
      "The tt optos vazived ifaroup we vicopllo the. nistpal thinc on that, inblm bfesolbt ill' og, thaplein'd iiy dom wrothed potrat bit hone Ceal,\n",
      " \n",
      "----\n",
      "iter 3600, loss: 59.350283\n",
      "----\n",
      " Nazy ocor be sone, then ar io asd faths sitpengud I, ue st cimel he lis Rot sout shiqeoch, sou' hetred. nom; yel\n",
      "\n",
      "BRoLir,\n",
      "\n",
      "CORUNUS:\n",
      "Ae.\n",
      "\n",
      "MENENENIUS:\n",
      "Cis coll:\n",
      "Lbcle.\n",
      "\n",
      "CIAIUS:\n",
      "He towe hones ho buce?\n",
      "\n",
      "C \n",
      "----\n",
      "iter 3700, loss: 58.916807\n",
      "----\n",
      "  ure- no kitian chmacn the.\n",
      "\n",
      "CORINLUS:\n",
      "And seremaplt,\n",
      "The mo nole our:\n",
      "Thikeg fures,\n",
      "No sirthes men, outh the fCid dong n'y\n",
      "unid Weria sor mimerelall, af trines\n",
      "bit on nous sainot, ow; belk's:\n",
      "He ate, \n",
      "----\n",
      "iter 3800, loss: 58.463649\n",
      "----\n",
      " esyou an; heapcile wit morr trall meurtsisein peaves oues, itum paveneles onguy thoce youem ourer-\n",
      "I soun to wothem wiss bhirerpleryou\n",
      "'osens ciles wr matd coukirelh bumc youm\n",
      "And th more memennusto p \n",
      "----\n",
      "iter 3900, loss: 58.304435\n",
      "----\n",
      " shin sut gark.\n",
      "\n",
      "The ferink, ermit, bit thy youre ofrep.\n",
      "Whe.\n",
      "\n",
      "VORUIOCIA:\n",
      "Ofale'w ap bathel sateth mous at hour y of.\n",
      "\n",
      "CININIUS:\n",
      "Od bh, ar st murt you mistuom geess.\n",
      "Hether mithe'-\n",
      "Ffisey: I ho ly morr \n",
      "----\n",
      "iter 4000, loss: 57.890550\n",
      "----\n",
      " evzam sitid.\n",
      "An aces, oms great ellhit dey:\n",
      "Not af tor ay so ive osn, if thata that pamg somidco the wowe.\n",
      "In ve?\n",
      "I mius', anthe'en thua prall\n",
      "Aonhs'us, coin mathuu, ange tom: thin in comen leall! nol \n",
      "----\n",
      "iter 4100, loss: 57.624998\n",
      "----\n",
      " e then tuys wpitut,\n",
      "Whon:\n",
      "Pon aredy.\n",
      "\n",
      "CORIOLANUS:\n",
      "I tous re, sorreryeland engeisdsthe to do hamy hor pill\n",
      "Snour:\n",
      "\n",
      "OLAES:\n",
      "On le thonou sile mide wive sant stere Coas, blestigoth havehr, lonse gunesgt r \n",
      "----\n",
      "iter 4200, loss: 57.544022\n",
      "----\n",
      " urme.\n",
      "\n",
      "Hous thiy :\n",
      "Dome?\n",
      "\n",
      "OmNIUS:\n",
      "\n",
      "VOVhaze: 'vakres, bokavey genstt katnell, the drinn. I And yhou man:t\n",
      "\n",
      "VOLIOLUTUS:\n",
      "I Ir the, scis buterevea dert we thase beetsee lor:\n",
      "As?\n",
      "\n",
      "AOLzNI wh Tyom Foustir so \n",
      "----\n",
      "iter 4300, loss: 57.168485\n",
      "----\n",
      " ut sin- meem, shay!\n",
      "dy hire:\n",
      "Tholls nofe fucu houlby robl: Sir;;\n",
      "Tom nititt in as at iuve  ald mansp; and s'ame; tou ars thoh atit; war hir lo wato buss,\n",
      "Wo\n",
      "cy is nome fome lat thind mo the fo the cin \n",
      "----\n",
      "iter 4400, loss: 57.102454\n",
      "----\n",
      " ve;\n",
      "Sodes cay.\n",
      "\n",
      "Fom:\n",
      "Mat tinc; I Seatpt to thescee ther fome wurse sgich:\n",
      "A\n",
      "MICIUS:\n",
      "For olve nomit I's mast: I thedisel, sishre;\n",
      "\n",
      "MENILICINIUS:\n",
      "Wher fitece mas?\n",
      "Tha herves yip moure\n",
      "Ham me ber hore de \n",
      "----\n",
      "iter 4500, loss: 57.101988\n",
      "----\n",
      " ? then:\n",
      "WheeRellelV hith uxtey y th the anwert bery, apes aw fonger to unerve, ho enes on ains misee ima; bo be rour anas an, thar tomar skith in an ith stpilf this;'d madsn: esver halnce I wind he wa \n",
      "----\n",
      "iter 4600, loss: 57.021624\n",
      "----\n",
      " nd mares thee:\n",
      "\n",
      "Thin:\n",
      "Siy thia tomenext Seoras thr:\n",
      "Thibp tare soous ingsen'de,\n",
      "And and wawy ar inds'se'd\n",
      "Heit luns the ares ounos\n",
      "yoe kan ditgwit.\n",
      "Whe t woledsat.\n",
      "\n",
      "SININIUS:\n",
      "Thes\n",
      "Sof sure there to mo \n",
      "----\n",
      "iter 4700, loss: 56.878971\n",
      "----\n",
      " stinnd in walld.\n",
      "\n",
      "SIRINENIUS:\n",
      "ougi gume, iy se wobet, adiu.\n",
      "\n",
      "Son: Iurs; thiothen yard by doond, corns.\n",
      "Spedmy rpy, I no anghen you thaveat ipe souke:\n",
      "Yo it popll te ale ind ius.\n",
      "\n",
      "VORIoL IUS:\n",
      "FoRent co \n",
      "----\n",
      "iter 4800, loss: 56.665575\n",
      "----\n",
      "  soireci? nor theth kehzow?\n",
      "YoWt eat and wnear mume huncaeld us, womin doek,,\n",
      "Whad\n",
      "We? ar hins.\n",
      "Ho? thaelr soln besniu te thor ous.\n",
      "\n",
      "\n",
      "MENENINIUS:\n",
      "Dhike fisc wim I mis noude'd\n",
      "in has hicust tha prey do \n",
      "----\n",
      "iter 4900, loss: 56.581173\n"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print ('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
